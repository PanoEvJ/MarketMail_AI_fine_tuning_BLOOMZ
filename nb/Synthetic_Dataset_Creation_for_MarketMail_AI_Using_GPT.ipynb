{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Access\n",
        "\n",
        "First things first, you'll need to set-up an account on [OpenAI](platform.openai.com). Once you've done that - follow [these resources](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to create an API key. Make sure you save your API key!"
      ],
      "metadata": {
        "id": "saLMqCm7huKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktti2bvoYrWc"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "# Set the OPENAI_API_KEY environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API Library\n",
        "\n",
        "We'll be leveraging [this](https://github.com/openai/openai-python) library to access OpenAI's model endpoints.\n",
        "\n",
        "There are a number of models to choose from and you can find resources about them [here](https://platform.openai.com/docs/models) and their pricing [here](https://openai.com/pricing).\n",
        "\n",
        "The first step is to install `openai`!"
      ],
      "metadata": {
        "id": "jzK0jIw8tgjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "UpeXOOlfqZCb",
        "outputId": "3d46f56f-3a65-4da2-e8b4-6d1aacbc6c93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've installed it, we need to import it and set our API key!"
      ],
      "metadata": {
        "id": "lM7zC-wPuxgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai \n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "SrdunVxHuQbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wanted to use `gpt-4`, you'd need an account that has closed beta access to the model endpoint. \n",
        "\n",
        "You can check if your API Key has access using the following cell."
      ],
      "metadata": {
        "id": "HB9Syy4Ku59z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if acct. has gpt-4 access\n",
        "\"gpt-4\" in [model[\"root\"] for model in openai.Model.list()[\"data\"]]"
      ],
      "metadata": {
        "id": "pOLxFHowvQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the rest of the tutorial, we're going to assume you're using `gpt-3.5-turbo` as your model.\n",
        "\n",
        "Let's make some helper functions for prompting our model and generating our prompts."
      ],
      "metadata": {
        "id": "XLzL4KLtS74x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_model(prompt_list, model=\"gpt-3.5-turbo\"):\n",
        "  return openai.ChatCompletion.create(model=model, messages=prompt_list)\n",
        "\n",
        "def create_prompt(role, prompt):\n",
        "  return {\"role\" : role, \"content\" : prompt}"
      ],
      "metadata": {
        "id": "AX3FWxWSS7Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our prompts have to be in a specific format - as set by OpenAI.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "```\n",
        "{\"role\" : \"system\", \"content\" : \"You are an expert in Python programming.\"}\n",
        "{\"role\" : \"user\", \"content\" : \"Please define a function that provides the Nth number of the fibonacci sequence.\"}\n",
        "```\n",
        "\n",
        "Let's see that in action! Remember that you can feed OpenAI's chat completion endpoint with a list of prompts!"
      ],
      "metadata": {
        "id": "CS7BCtXuURf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    {\"role\" : \"system\", \"content\" : \"You are an expert in Python programming.\"}, \n",
        "    {\"role\" : \"user\", \"content\" : \"Please define a function that provides the Nth number of the fibonacci sequence.\"}\n",
        "]\n",
        "\n",
        "model_output = prompt_model(list_of_prompts)\n",
        "print(model_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js3DlcsBUrl8",
        "outputId": "22500f3b-c605-4bde-bc91-a36884cbce34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"Sure! Here's a function that returns the Nth number in the Fibonacci sequence using recursion:\\n\\n```python\\ndef fibonacci(n):\\n    if n <= 0:\\n        return \\\"Invalid input, n should be greater than or equal to 1\\\"\\n    elif n == 1:\\n        return 0\\n    elif n == 2:\\n        return 1\\n    else:\\n        return fibonacci(n - 1) + fibonacci(n - 2)\\n\\n# Example Usage:\\nn = 10\\nprint(f\\\"The {n}th number in the Fibonacci sequence is {fibonacci(n)}\\\")\\n```\\n\\nPlease note that this implementation is not optimized for large values of `n` as it has an exponential time complexity. For larger values of `n`, I recommend using dynamic programming or matrix exponentiation to solve the problem.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1682484371,\n",
            "  \"id\": \"chatcmpl-79RKdT6EXbI3JlXY6bKb9IBfIARfh\",\n",
            "  \"model\": \"gpt-4-0314\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 166,\n",
            "    \"prompt_tokens\": 34,\n",
            "    \"total_tokens\": 200\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we get a lot of information back from the endpoint. \n",
        "\n",
        "We can see the number of tokens we used, why the output stopped, what the output is, and more!\n",
        "\n",
        "Let's view the prompt a bit clearer using some display libraries. "
      ],
      "metadata": {
        "id": "tC3EcGQxVWID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "markdown_output = model_output[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "display(Markdown(markdown_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "H9z97v9eVnvn",
        "outputId": "3c0bf2f5-18ef-4ed4-dafb-75bb7b177abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure! Here's a function that returns the Nth number in the Fibonacci sequence using recursion:\n\n```python\ndef fibonacci(n):\n    if n <= 0:\n        return \"Invalid input, n should be greater than or equal to 1\"\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\n# Example Usage:\nn = 10\nprint(f\"The {n}th number in the Fibonacci sequence is {fibonacci(n)}\")\n```\n\nPlease note that this implementation is not optimized for large values of `n` as it has an exponential time complexity. For larger values of `n`, I recommend using dynamic programming or matrix exponentiation to solve the problem."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Synthetic Data\n",
        "\n",
        "Alright, now we can pull everything together and start creating our synthetic data!\n",
        "\n",
        "**NOTE:** Using OpenAI's endpoints to create our dataset does mean that we cannot use our model for commercial use. This is meant to demonstrate the methods, and can be extended to any open-source LLM."
      ],
      "metadata": {
        "id": "AKPMtZXKWnj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use this process to create 100 product/marketing email pairs. \n",
        "\n",
        "We'll be doing this in 2-steps:\n",
        "\n",
        "1. Create the 100 products and short descriptions.\n",
        "2. Create marketing emails for each of those 100 product/descriptions.\n",
        "\n",
        "Let's begin by creating the prompt for our products/descriptions!"
      ],
      "metadata": {
        "id": "-t2gQB22W7yF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "audN435yYrWd"
      },
      "outputs": [],
      "source": [
        "datagen_prompts = [\n",
        "    {\"role\" : \"system\", \"content\" : \"You are a product innovator. You create new products that people crave.\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"Please generate a list of 10 new products and extremely short descriptions.\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUJPivDkYrWd",
        "outputId": "8c549ef3-8ebf-4fb6-d87b-165d02216487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are 10 new product ideas:\n",
            "\n",
            "1. Mind Spa: A personal device that provides calming and relaxing sounds to help people unwind and release stress.\n",
            "\n",
            "2. Smart Bottle: A bottle that tracks the hydration level of its user and reminds them to drink water throughout the day.\n",
            "\n",
            "3. Mood ring smartwatch: A watch that detects the emotions of its wearer and suggests activities to improve their mood.\n",
            "\n",
            "4. Virtual Closet: An app that helps users digitally organize and plan their outfits for the week, reducing the time and stress of choosing clothes.\n",
            "\n",
            "5. Energy patch: A wearable patch that delivers caffeine and other natural stimulants to increase energy and focus.\n",
            "\n",
            "6. Smart fridge magnet: A fridge magnet that detects when food is running low and automatically orders more from your local grocery store.\n",
            "\n",
            "7. Water purifier bottle: A bottle that filters tap water into drinking water, making it convenient for people who are always on the go or travel frequently.\n",
            "\n",
            "8. Smart odor remover: A device that detects and eliminates bad odors in a room or car, improving air quality and refreshing the environment.\n",
            "\n",
            "9. Portable gym: A portable exercise equipment that allows people to work out anywhere, anytime, without the need for a gym membership.\n",
            "\n",
            "10. Smart measuring cup: A measuring cup that accurately measures ingredients and provides recipe suggestions based on what's in your pantry.\n"
          ]
        }
      ],
      "source": [
        "first_data_gen = prompt_model(datagen_prompts)\n",
        "print(first_data_gen[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now that we have a list of 100 items - let's parse them out into a Python list - also, we can keep track of our total token usage to estimate costs!"
      ],
      "metadata": {
        "id": "rm2m394F5eRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_token_usage(open_ai_response):\n",
        "  return sum([tokens for tokens in open_ai_response[\"usage\"].values()])"
      ],
      "metadata": {
        "id": "QRNcGdFf5lYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"We used {retrieve_token_usage(first_data_gen)} tokens\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aNh3SrsI6Aup",
        "outputId": "50d99f7b-0150-410e-db23-2fb73acfa3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We used 824 tokens'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code might need to be modified based on how your data was returned by OpenAI's endpoint!"
      ],
      "metadata": {
        "id": "7g6-Ywiw7JjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_response = first_data_gen[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "products_and_descriptions = []\n",
        "for line in text_response.splitlines():\n",
        "  if \".\" in line:\n",
        "    product_descriptions = line.split(\".\")[1]\n",
        "    product_descriptions_split = product_descriptions.split(\":\")\n",
        "    products_and_descriptions.append(\n",
        "        {\n",
        "            \"product\" : product_descriptions_split[0][1:], \n",
        "            \"description\" : \":\".join(product_descriptions_split[1:])[1:]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "p6iTuZzx6fWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_and_descriptions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy9zrlXV7uMS",
        "outputId": "be799727-f768-4191-df91-b670a6341c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'Mind Spa',\n",
              " 'description': 'A personal device that provides calming and relaxing sounds to help people unwind and release stress'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our items parsed out into a Python list - we can go ahead and iterate through each of the items and have whichever OpenAI model you selected create a short marketing email for it!\n",
        "\n",
        "First though, we'll need a system prompt to use!"
      ],
      "metadata": {
        "id": "sLTporSY8QNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = create_prompt(\n",
        "    \"system\", \n",
        "    \"You are a marketing executive. You are proficient at writing short, and snappy marketing emails. The emails should be easy to read, and contain excited and vibrant language.\"\n",
        ")"
      ],
      "metadata": {
        "id": "GIu0qZzJ9dPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need a user prompt - we'll have to wrap this in a function so we can call it for each item of the 100 items we created above."
      ],
      "metadata": {
        "id": "5sQH5-BN-DAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_user_prompt(product, description):\n",
        "  user_prompt = create_prompt(\n",
        "      \"user\",\n",
        "      f\"Please create a short marketing email using this product: {product} and this description: {description}\"\n",
        "  )\n",
        "\n",
        "  return user_prompt"
      ],
      "metadata": {
        "id": "pbF9esxS-KK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're good to start generating our synthetic data! We simply need to iterate through each item - and collate the results into a list of dictionaries!\n",
        "\n",
        "(depending on which model you use, this step might take a long time, and could become expensive!)"
      ],
      "metadata": {
        "id": "tqi_ax8o-iJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.error import RateLimitError\n",
        "total_token_usage = 0\n",
        "\n",
        "for idx, item in enumerate(products_and_descriptions):\n",
        "  if \"marketing_email\" in item:\n",
        "    continue\n",
        "  print(f\"Working on {idx}\")\n",
        "  user_prompt = generate_user_prompt(item[\"product\"], item[\"description\"])\n",
        "  full_prompt = [system_prompt, user_prompt]\n",
        "  try: \n",
        "    prompt_response = prompt_model(full_prompt)\n",
        "    item[\"marketing_email\"] = prompt_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    total_token_usage += retrieve_token_usage(prompt_response)\n",
        "  except RateLimitError as e:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBx7PGNW-raA",
        "outputId": "2c42fecd-182e-4a15-8e60-b5935611eb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on 0\n",
            "Working on 1\n",
            "Working on 2\n",
            "Working on 3\n",
            "Working on 4\n",
            "Working on 5\n",
            "Working on 6\n",
            "Working on 7\n",
            "Working on 8\n",
            "Working on 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "products_desc_and_marktng_emails_dataset = [p_d_and_m for p_d_and_m in products_and_descriptions if \"marketing_email\" in p_d_and_m]"
      ],
      "metadata": {
        "id": "4gTKNHr1YxpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Dataset to HuggingFace Hub\n",
        "\n",
        "Now that we've created our synthetic dataset - let's push it to the HuggingFace hub!\n",
        "\n",
        "As always, the first task is to get the required dependencies."
      ],
      "metadata": {
        "id": "M2DwQSnfX3YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5HrUWPXoU7k",
        "outputId": "991b5321-24f3-4a0d-f860-663dc2d6b46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/224.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can log-in to Hugging Face!\n",
        "\n",
        "Make sure you have a Hugging Face account, and you have set up a read/write token!\n",
        "\n",
        "More info here: https://huggingface.co/docs/hub/security-tokens"
      ],
      "metadata": {
        "id": "AsYiou0LoiyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "i_fslrEAocXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1864d969-db0c-4bab-e4a6-1dc12e4f4154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load our data into the desired format - and upload it to the hub!"
      ],
      "metadata": {
        "id": "dzcw9DnAo1u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "k-W6NSjrrSt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b03074-e594-4d2c-e3e0-cbc6d3aa3dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "319x0ZlJrNrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset = Dataset.from_pandas(pd.DataFrame(data=products_desc_and_marktng_emails_dataset))"
      ],
      "metadata": {
        "id": "ohM2ANNFrYxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOQgiUutxH8a",
        "outputId": "d374d00d-b4c3-4d78-e967-6b7f927bcd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['product', 'description', 'marketing_email'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_username = \"<<YOUR USERNAME HERE>>\"\n",
        "dataset_name = \"<<YOUR DATASET NAME HERE>>\"\n",
        "\n",
        "hf_dataset.push_to_hub(f\"{hf_username}/{dataset_name}\")"
      ],
      "metadata": {
        "id": "_wRUnFg0xIy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "And that's it! You just created a synthetic dataset and pushed it to the hub! \n",
        "\n",
        "Next stop? [Modeling!](https://colab.research.google.com/drive/1RfUuzG11Q8AaZuJIHLzXCVC087xoDeSd?usp=sharing)"
      ],
      "metadata": {
        "id": "WNkaJ9m4ZP6B"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "open_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}